"""
å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

MarS Liteç’°å¢ƒã§PPOã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å­¦ç¿’
å¤šæ™‚é–“è»¸ãƒ»ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ»train/valåˆ†å‰²ãƒ»è¤‡æ•°é€šè²¨å¯¾å¿œ
"""

import argparse
from pathlib import Path
import json
import numpy as np
import pandas as pd
from typing import Dict, List

from mars_lite.data.preprocessing import preprocess_ohlcv
from mars_lite.data.multi_timeframe_loader import MultiTimeframeLoader, MultiSymbolLoader
from mars_lite.data.data_split import split_temporal, split_temporal_multi_tf, get_split_info
from mars_lite.env.mars_lite_env import MarsLiteEnv
# from mars_lite.env.multi_tf_env import MarsLiteMultiTFEnv
from mars_lite.env.cross_symbol_env import CrossSymbolEnv, SequentialSymbolEnv
from mars_lite.learning.agent import create_ppo_agent, train_agent, evaluate_agent
from mars_lite.learning.random_sampler import RandomEpisodeSampler, MultiModeEpisodeSampler
from mars_lite.learning.training_callback import (
    TrainingMetricsCallback,
    CheckpointCallback,
    get_metrics_history,
)
from mars_lite.utils.config import MarsLiteConfig, create_ppo_kwargs, create_env_kwargs

# ä¸Šä½30é€šè²¨ï¼ˆfetch_binance.pyã¨åŒã˜ãƒªã‚¹ãƒˆï¼‰

def load_available_symbols(data_dir: Path) -> List[str]:
    """
    ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰åˆ©ç”¨å¯èƒ½ãªã‚·ãƒ³ãƒœãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—
    metadata.jsonãŒã‚ã‚Œã°ãã‚Œã‚’å„ªå…ˆã€ãªã‘ã‚Œã°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¹ã‚­ãƒ£ãƒ³
    """
    metadata_path = data_dir / "metadata.json"
    if metadata_path.exists():
        try:
            with open(metadata_path, "r", encoding="utf-8") as f:
                metadata = json.load(f)
                symbols = metadata.get("symbols", [])
                if symbols:
                    return symbols
        except Exception as e:
            print(f"Warning: Failed to load metadata.json: {e}")

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¹ã‚­ãƒ£ãƒ³
    symbols = []
    if data_dir.exists():
        for item in data_dir.iterdir():
            if item.is_dir() and item.name.endswith("USDT"):
                symbols.append(item.name)
    return sorted(symbols)



def load_single_tf_data(data_path: str) -> pd.DataFrame:
    """
    å˜ä¸€æ™‚é–“è»¸OHLCVãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ãƒ»å‰å‡¦ç†
    
    Args:
        data_path: CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        
    Returns:
        å‰å‡¦ç†æ¸ˆã¿DataFrame
    """
    df = pd.read_csv(data_path)
    df.columns = df.columns.str.lower()
    
    if "timestamp" in df.columns:
        df["timestamp"] = pd.to_datetime(df["timestamp"])
    elif "date" in df.columns:
        df["timestamp"] = pd.to_datetime(df["date"])
        df = df.drop(columns=["date"])
    
    return preprocess_ohlcv(df)


def load_multi_tf_data(
    data_dir: str,
    config: MarsLiteConfig,
    start_date: str = None,
    end_date: str = None,
    limit_days: int = None,
) -> tuple:
    """
    å¤šæ™‚é–“è»¸ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼ˆæ—¥åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼å¯¾å¿œï¼‰
    
    Args:
        data_dir: ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        config: è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        start_date: é–‹å§‹æ—¥ YYYY-MM-DD
        end_date: çµ‚äº†æ—¥ YYYY-MM-DD
        limit_days: æœ€å¤§æ—¥æ•°
        
    Returns:
        (base_data, higher_tf_data)
    """
    loader = MultiTimeframeLoader(
        data_dir=Path(data_dir),
        timeframes=list(config.timeframes),
        symbol=config.symbol,
        days=limit_days or config.data_days,
        preprocess=True,
    )
    
    loader.load_all(
        start_date=start_date,
        end_date=end_date,
        limit_days=limit_days,
    )
    base_data = loader.get_base_timeframe()
    higher_tf_data = loader.get_higher_timeframes()
    
    print(f"  ãƒ‡ãƒ¼ã‚¿å½¢å¼: {'æ—¥åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«' if loader.is_daily_format else 'å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«'}")
    
    return base_data, higher_tf_data


def load_multi_symbol_data(
    data_dir: str,
    symbols: List[str],
    config: MarsLiteConfig,
    start_date: str = None,
    end_date: str = None,
    limit_days: int = None,
) -> Dict[str, tuple]:
    """
    è¤‡æ•°é€šè²¨ã®å¤šæ™‚é–“è»¸ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    
    Args:
        data_dir: ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        symbols: é€šè²¨ãƒªã‚¹ãƒˆ
        config: è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        start_date: é–‹å§‹æ—¥
        end_date: çµ‚äº†æ—¥
        limit_days: æœ€å¤§æ—¥æ•°
       
    Returns:
        {symbol: (base_data, higher_tf_data)}ã®è¾æ›¸
    """
    loader = MultiSymbolLoader(
        data_dir=Path(data_dir),
        symbols=symbols,
        timeframes=list(config.timeframes),
        days=limit_days or config.data_days,
        preprocess=True,
    )
    
    all_data = loader.load_all(
        start_date=start_date,
        end_date=end_date,
        limit_days=limit_days,
    )
    
    result = {}
    for symbol in loader.loaded_symbols:
        loader_obj = loader.get_loader(symbol)
        result[symbol] = (
            loader_obj.get_base_timeframe(),
            loader_obj.get_higher_timeframes(),
        )
    
    print(f"  èª­ã¿è¾¼ã¿æˆåŠŸ: {len(result)}/{len(symbols)}é€šè²¨")
    
    return result


def create_sample_data(n_bars: int = 10000) -> pd.DataFrame:
    """
    ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆï¼‰
    
    Args:
        n_bars: ãƒãƒ¼æ•°
        
    Returns:
        å‰å‡¦ç†æ¸ˆã¿DataFrame
    """
    np.random.seed(42)
    
    # ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ã§ä¾¡æ ¼ç”Ÿæˆ
    returns = np.random.randn(n_bars) * 0.002
    close = 100 * np.exp(np.cumsum(returns))
    
    # OHLCç”Ÿæˆ
    high = close * (1 + np.abs(np.random.randn(n_bars)) * 0.003)
    low = close * (1 - np.abs(np.random.randn(n_bars)) * 0.003)
    open_ = low + (high - low) * np.random.rand(n_bars)
    
    # å‡ºæ¥é«˜ï¼ˆæ™‚åˆ»ä¾å­˜ã®Uå­—å‹ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
    tod = np.arange(n_bars) % 1440
    base_volume = 1000 * (1 + 0.5 * np.cos(2 * np.pi * tod / 1440))
    volume = base_volume * np.random.exponential(1, n_bars)
    
    timestamps = pd.date_range("2024-01-01", periods=n_bars, freq="1min")
    
    df = pd.DataFrame({
        "timestamp": timestamps,
        "open": open_,
        "high": high,
        "low": low,
        "close": close,
        "volume": volume,
    })
    
    return preprocess_ohlcv(df)


def create_env_with_sampler(
    data: pd.DataFrame,
    higher_tf_data: dict,
    config: MarsLiteConfig,
    sampler: RandomEpisodeSampler = None,
) -> MarsLiteEnv:
    """
    ç’°å¢ƒã‚’ä½œæˆï¼ˆMarsLiteEnvã«çµ±ä¸€ï¼‰
    
    Args:
        data: ãƒ™ãƒ¼ã‚¹æ™‚é–“è»¸ãƒ‡ãƒ¼ã‚¿
        higher_tf_data: ä¸Šä½æ™‚é–“è»¸ãƒ‡ãƒ¼ã‚¿
        config: è¨­å®š
        sampler: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚µãƒ³ãƒ—ãƒ©ãƒ¼ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        
    Returns:
        ç’°å¢ƒã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    """
    env_kwargs = create_env_kwargs(config)
    
    # data_dictã‚’å†æ§‹ç¯‰
    # MarsLiteEnvã¯ {symbol: {tf: df}} ã‚’æœŸå¾…ã™ã‚‹ãŒã€
    # ã“ã“ã§ã¯å˜ä¸€é€šè²¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ¸¡ã•ã‚Œã¦ã„ã‚‹å‰æï¼ˆCrossSymbolEnvã§ãƒ©ãƒƒãƒ—ã•ã‚Œã‚‹å‰ï¼‰
    # ã—ã‹ã—ã€MarsLiteEnvã¯å†…éƒ¨ã§ `data_dict[current_symbol]` ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã€‚
    # ã“ã“ã§ã¯ä¾¿å®œä¸Šã€config.symbolã‚’ã‚­ãƒ¼ã¨ã—ã¦ data_dict ã‚’ä½œæˆã™ã‚‹ã€‚
    
    symbol = config.symbol or "UNKNOWN"
    
    # higher_tf_data ã«ã¯ '15m', '1h' ãªã©ãŒå…¥ã£ã¦ã„ã‚‹ã€‚
    # base_timeframe ('1m') ã‚‚å«ã‚ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚
    tf_data = {config.base_timeframe: data}
    if higher_tf_data:
        tf_data.update(higher_tf_data)
        
    data_dict = {symbol: tf_data}
    
    # ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ãƒªã‚¹ãƒˆã®æ§‹ç¯‰
    # config.timeframes ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ã†ãŒã€data_dictã«ã‚ã‚‹ã‚‚ã®ã‚’å„ªå…ˆã™ã¹ãã‹ï¼Ÿ
    # MarsLiteEnvã¯ config.timeframes ã®é †åºã‚’æœŸå¾…ã™ã‚‹ã€‚
    # æ¬ æãƒã‚§ãƒƒã‚¯ã¯Envå†…ã§è¡Œã‚ã‚Œã‚‹ã€‚
    
    # higher_tf_lookback ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ MarsLiteEnv ã«ã¯ãªã„ï¼ˆobsæ§‹æˆãŒé•ã†ãŸã‚ç„¡è¦–ã•ã‚Œã‚‹ã‹ã€çµ±åˆæ¸ˆã¿ï¼‰
    # MarsLiteEnvã® __init__ ã‚’ç¢ºèªã™ã‚‹ã¨:
    # n_lookback, y_impact, lambda_risk ãªã©ã‚’å—ã‘å–ã‚‹ã€‚
    # higher_tf_lookback ã¯å—ã‘å–ã‚‰ãªã„ï¼ˆå›ºå®šé•·ã‹ã€å†…éƒ¨ãƒ­ã‚¸ãƒƒã‚¯ï¼‰ã€‚
    # ãªã®ã§ä¸è¦ãªkwargsã‚’é™¤å»ã™ã‚‹ã€‚
    
    safe_kwargs = {k: v for k, v in env_kwargs.items() if k not in ["higher_tf_lookback"]}
    
    env = MarsLiteEnv(
        data_dict=data_dict,
        timeframes=list(config.timeframes),
        **safe_kwargs
    )
    
    return env


def main():
    parser = argparse.ArgumentParser(description="MarS Lite å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆè¤‡æ•°é€šè²¨ãƒ»å¤šæ™‚é–“è»¸ï¼‰")
    parser.add_argument("--data", type=str, default=None, help="ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ï¼ˆCSVã¾ãŸã¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼‰")
    parser.add_argument("--symbol", type=str, default=None, help="é€šè²¨ãƒšã‚¢ï¼ˆå˜ä¸€é€šè²¨æ™‚ï¼‰")
    parser.add_argument("--symbols", type=str, nargs="+", default=None, help="è¤‡æ•°é€šè²¨ãƒšã‚¢")
    parser.add_argument("--top", type=int, default=None, help="ä¸Šä½Né€šè²¨")
    parser.add_argument("--start-date", type=str, default=None, help="é–‹å§‹æ—¥ YYYY-MM-DD")
    parser.add_argument("--end-date", type=str, default=None, help="çµ‚äº†æ—¥ YYYY-MM-DD")
    parser.add_argument("--limit-days", type=int, default=None, help="æœ€å¤§æ—¥æ•°")
    parser.add_argument("--all", action="store_true", dest="use_all_data", help="å…¨æœŸé–“ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨")
    parser.add_argument("--config", type=str, default=None, help="è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆJSONï¼‰")
    parser.add_argument("--output", type=str, default="./output", help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument("--timesteps", type=int, default=100000, help="ç·å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—æ•°")
    parser.add_argument("--seed", type=int, default=42, help="ä¹±æ•°ã‚·ãƒ¼ãƒ‰")
    parser.add_argument("--verbose", type=int, default=1, help="ãƒ­ã‚°å‡ºåŠ›ãƒ¬ãƒ™ãƒ«")
    parser.add_argument("--multi-tf", action="store_true", help="å¤šæ™‚é–“è»¸ãƒ¢ãƒ¼ãƒ‰")
    parser.add_argument("--no-split", action="store_true", help="ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ã‚’ç„¡åŠ¹åŒ–")
    parser.add_argument("--serve", action="store_true", help="UIã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•")
    parser.add_argument("--port", type=int, default=8000, help="UIã‚µãƒ¼ãƒãƒ¼ãƒãƒ¼ãƒˆ")
    parser.add_argument("--checkpoint-freq", type=int, default=10000, help="ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜é–“éš”")
    args = parser.parse_args()
    
    # è¤‡æ•°é€šè²¨ãƒªã‚¹ãƒˆã®æ±ºå®š
    # ã¾ãšãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰åˆ©ç”¨å¯èƒ½ãªã‚·ãƒ³ãƒœãƒ«ã‚’å–å¾—
    available_symbols = []
    if args.data:
        data_path = Path(args.data)
        if data_path.is_dir():
            available_symbols = load_available_symbols(data_path)
            
    if args.symbols:
        symbols = args.symbols
        multi_symbol_mode = True
    elif args.top:
        if available_symbols:
             symbols = available_symbols[:args.top]
        else:
             print("Warning: No data found to select top symbols from. Using fallback list.")
             # Fallback list (Historical Top 10)
             symbols = ["BTCUSDT", "ETHUSDT", "BNBUSDT", "SOLUSDT", "XRPUSDT", "ADAUSDT", "DOGEUSDT", "TRXUSDT", "AVAXUSDT", "LINKUSDT"][:args.top]
        multi_symbol_mode = True
    elif args.symbol:
        symbols = [args.symbol]
        multi_symbol_mode = False
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: åˆ©ç”¨å¯èƒ½ãªå…¨ã‚·ãƒ³ãƒœãƒ«ã€ã¾ãŸã¯BTC
        if available_symbols:
            symbols = available_symbols
            multi_symbol_mode = True
            print(f"Using all available symbols from data directory: {len(symbols)} found.")
        else:
            symbols = ["BTCUSDT"]
            multi_symbol_mode = False
    
    # --all ãƒ•ãƒ©ã‚°å‡¦ç†
    if args.use_all_data:
        args.start_date = None
        args.limit_days = None
        print("\nâœ¨ å…¨æœŸé–“ãƒ¢ãƒ¼ãƒ‰: åˆ©ç”¨å¯èƒ½ãªå…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™")
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # è¨­å®šèª­ã¿è¾¼ã¿
    if args.config:
        config = MarsLiteConfig.load(args.config)
    else:
        config = MarsLiteConfig()
    
    config.seed = args.seed
    config.total_timesteps = args.timesteps
    config.save_dir = str(output_dir)
    config.symbol = symbols[0]  # ãƒ¡ã‚¤ãƒ³é€šè²¨ï¼ˆæœ€åˆã®é€šè²¨ï¼‰
    
    if args.multi_tf:
        config.use_multi_tf = True
    
    # è¨­å®šä¿å­˜
    config.save(str(output_dir / "config.json"))
    
    print("=" * 60)
    print("MarS Lite å­¦ç¿’é–‹å§‹ï¼ˆè¤‡æ•°é€šè²¨ãƒ»å¤šæ™‚é–“è»¸ï¼‰")
    print("=" * 60)
    if multi_symbol_mode:
        print(f"é€šè²¨: {len(symbols)}é€šè²¨ {symbols[:5]}{'...' if len(symbols) > 5 else ''}")
    else:
        print(f"é€šè²¨: {symbols[0]}")
    print(f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")
    print(f"ç·ã‚¹ãƒ†ãƒƒãƒ—æ•°: {config.total_timesteps:,}")
    print(f"å¤šæ™‚é–“è»¸ãƒ¢ãƒ¼ãƒ‰: {config.use_multi_tf}")
    print(f"æ™‚é–“è»¸: {config.timeframes}")
    print("=" * 60)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    multi_symbol_data = {}  # {symbol: (base_data, higher_tf_data)}
    
    if args.data:
        data_path = Path(args.data)
        
        if data_path.is_dir():
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæŒ‡å®š: å¤šæ™‚é–“è»¸/æ—¥åˆ¥ãƒ‡ãƒ¼ã‚¿
            print(f"\\nãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­: {data_path}")
            config.data_dir = str(data_path)
            
            if multi_symbol_mode:
                # è¤‡æ•°é€šè²¨èª­ã¿è¾¼ã¿
                multi_symbol_data = load_multi_symbol_data(
                    str(data_path),
                    symbols,
                    config,
                    start_date=args.start_date,
                    end_date=args.end_date,
                    limit_days=args.limit_days,
                )
                # æœ€åˆã®é€šè²¨ã‚’ãƒ¡ã‚¤ãƒ³è¡¨ç¤º
                if multi_symbol_data:
                    first_symbol = list(multi_symbol_data.keys())[0]
                    base_data, higher_tf_data = multi_symbol_data[first_symbol]
                    print(f"  {first_symbol} ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿: {len(base_data):,}ãƒãƒ¼")
            else:
                # å˜ä¸€é€šè²¨èª­ã¿è¾¼ã¿
                base_data, higher_tf_data = load_multi_tf_data(
                    str(data_path), 
                    config,
                    start_date=args.start_date,
                    end_date=args.end_date,
                    limit_days=args.limit_days,
                )
                print(f"ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(base_data):,}ãƒãƒ¼")
                if higher_tf_data:
                    print(f"ä¸Šä½TF: {list(higher_tf_data.keys())}")
        else:
            # ãƒ•ã‚¡ã‚¤ãƒ«æŒ‡å®š: å˜ä¸€æ™‚é–“è»¸
            print(f"å˜ä¸€æ™‚é–“è»¸ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­: {data_path}")
            base_data = load_single_tf_data(str(data_path))
            config.use_multi_tf = False
            higher_tf_data = {}
    else:
        print("ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆä¸­...")
        base_data = create_sample_data(n_bars=20000)
        config.use_multi_tf = False
        higher_tf_data = {}
    
    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    if not args.no_split:
        print("\nãƒ‡ãƒ¼ã‚¿åˆ†å‰²ä¸­...")
        
        if config.use_multi_tf and higher_tf_data:
            # å¤šæ™‚é–“è»¸ãƒ‡ãƒ¼ã‚¿ã‚‚åˆ†å‰²
            all_data = {config.base_timeframe: base_data, **higher_tf_data}
            train_data, val_data, test_data = split_temporal_multi_tf(
                all_data,
                train_ratio=config.train_ratio,
                val_ratio=config.val_ratio,
                test_ratio=config.test_ratio,
            )
            
            train_base = train_data[config.base_timeframe]
            val_base = val_data[config.base_timeframe]
            train_higher = {k: v for k, v in train_data.items() if k != config.base_timeframe}
            val_higher = {k: v for k, v in val_data.items() if k != config.base_timeframe}
        else:
            # å˜ä¸€æ™‚é–“è»¸
            train_base, val_base, test_base = split_temporal(
                base_data,
                train_ratio=config.train_ratio,
                val_ratio=config.val_ratio,
                test_ratio=config.test_ratio,
            )
            train_higher = {}
            val_higher = {}
        
        # åˆ†å‰²æƒ…å ±è¡¨ç¤º
        split_info = get_split_info(train_base, val_base, 
                                    test_data.get(config.base_timeframe, pd.DataFrame()) 
                                    if config.use_multi_tf else test_base)
        print(f"  Train: {split_info['train']['bars']:,}ãƒãƒ¼ ({split_info['train']['ratio']*100:.1f}%)")
        print(f"  Val: {split_info['val']['bars']:,}ãƒãƒ¼ ({split_info['val']['ratio']*100:.1f}%)")
        print(f"  Test: {split_info['test']['bars']:,}ãƒãƒ¼ ({split_info['test']['ratio']*100:.1f}%)")
        
        # åˆ†å‰²æƒ…å ±ä¿å­˜
        with open(output_dir / "split_info.json", "w", encoding="utf-8") as f:
            json.dump(split_info, f, indent=2, ensure_ascii=False)
    else:
        train_base = base_data
        val_base = base_data
        train_higher = higher_tf_data
        val_higher = higher_tf_data
    
    # ç’°å¢ƒä½œæˆ
    print("\nç’°å¢ƒã‚’ä½œæˆä¸­...")
    
    if multi_symbol_mode and multi_symbol_data:
        # è¤‡æ•°é€šè²¨ï¼šå„é€šè²¨ã”ã¨ã«ç’°å¢ƒã‚’ä½œæˆã—ã¦CrossSymbolEnvã§ãƒ©ãƒƒãƒ—
        train_envs = {}
        eval_envs = {}
        
        for symbol, (base, higher) in multi_symbol_data.items():
            # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
            if not args.no_split:
                if config.use_multi_tf and higher:
                    all_d = {config.base_timeframe: base, **higher}
                    train_d, val_d, _ = split_temporal_multi_tf(
                        all_d,
                        train_ratio=config.train_ratio,
                        val_ratio=config.val_ratio,
                        test_ratio=config.test_ratio,
                    )
                    train_b = train_d[config.base_timeframe]
                    val_b = val_d[config.base_timeframe]
                    train_h = {k: v for k, v in train_d.items() if k != config.base_timeframe}
                    val_h = {k: v for k, v in val_d.items() if k != config.base_timeframe}
                else:
                    train_b, val_b, _ = split_temporal(base, config.train_ratio, config.val_ratio, config.test_ratio)
                    train_h, val_h = {}, {}
            else:
                train_b, val_b = base, base
                train_h, val_h = higher, higher
            
            # ç’°å¢ƒä½œæˆ
            train_envs[symbol] = create_env_with_sampler(train_b, train_h, config)
            eval_envs[symbol] = create_env_with_sampler(val_b, val_h, config)
        
        # CrossSymbolEnvã§ãƒ©ãƒƒãƒ—
        train_env = CrossSymbolEnv(train_envs, seed=config.seed)
        eval_env = SequentialSymbolEnv(eval_envs)  # è©•ä¾¡ã¯é †ç•ªã«
        
        print(f"  é€šè²¨ã”ã¨ã®ç’°å¢ƒä½œæˆ: {len(train_envs)}ç’°å¢ƒ")
    else:
        # å˜ä¸€é€šè²¨ï¼šå¾“æ¥é€šã‚Š
        train_env = create_env_with_sampler(train_base, train_higher, config)
        eval_env = create_env_with_sampler(val_base, val_higher, config)
    
    print(f"è¦³æ¸¬ç©ºé–“: {train_env.observation_space.shape}")
    print(f"è¡Œå‹•ç©ºé–“: {train_env.action_space.shape}")
    
    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆ
    print("\nPPOã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½œæˆä¸­...")
    ppo_kwargs = create_ppo_kwargs(config)
    agent = create_ppo_agent(train_env, verbose=args.verbose, **ppo_kwargs)
    
    # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯æº–å‚™
    callbacks = []
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆå¸¸ã«æœ‰åŠ¹ï¼‰
    metrics_callback = TrainingMetricsCallback(
        total_timesteps=config.total_timesteps,
        log_freq=1,
        verbose=args.verbose,
    )
    callbacks.append(metrics_callback)
    
    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
    checkpoint_callback = CheckpointCallback(
        save_freq=args.checkpoint_freq,
        save_path=str(output_dir / "checkpoints"),
        name_prefix="model",
        verbose=args.verbose,
    )
    callbacks.append(checkpoint_callback)
    
    # UIã‚µãƒ¼ãƒãƒ¼èµ·å‹•ï¼ˆ--serveã‚ªãƒ—ã‚·ãƒ§ãƒ³æ™‚ï¼‰
    server_thread = None
    if args.serve:
        import threading
        from mars_lite.server.metrics_server import run_server
        
        def start_server():
            run_server(
                host="0.0.0.0",
                port=args.port,
                output_dir=str(output_dir),
            )
        
        server_thread = threading.Thread(target=start_server, daemon=True)
        server_thread.start()
        print(f"\nğŸ“¡ UIã‚µãƒ¼ãƒãƒ¼èµ·å‹•: http://localhost:{args.port}")
        print(f"   WebSocket: ws://localhost:{args.port}/ws/metrics")
    
    # å­¦ç¿’
    print("\nå­¦ç¿’é–‹å§‹...")
    agent = train_agent(
        agent=agent,
        total_timesteps=config.total_timesteps,
        callbacks=callbacks,
        eval_env=eval_env,
        eval_freq=10000,
        n_eval_episodes=5,
        save_path=str(output_dir),
    )
    
    # æœ€çµ‚è©•ä¾¡
    print("\næœ€çµ‚è©•ä¾¡ä¸­...")
    eval_results = evaluate_agent(agent, eval_env, n_episodes=10)
    
    print("=" * 60)
    print("å­¦ç¿’å®Œäº†")
    print("=" * 60)
    print(f"å¹³å‡å ±é…¬: {eval_results['mean_reward']:.4f} Â± {eval_results['std_reward']:.4f}")
    print(f"å¹³å‡ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é•·: {eval_results['mean_length']:.1f}")
    
    if eval_results["execution_stats"]:
        mean_trades = np.mean([s["n_trades"] for s in eval_results["execution_stats"]])
        mean_pov = np.mean([s["mean_pov"] for s in eval_results["execution_stats"]])
        print(f"å¹³å‡å–å¼•å›æ•°: {mean_trades:.1f}")
        print(f"å¹³å‡POV: {mean_pov:.4f}")
    
    # çµæœä¿å­˜
    with open(output_dir / "eval_results.json", "w", encoding="utf-8") as f:
        serializable = {
            k: float(v) if isinstance(v, (np.floating, np.integer)) else v
            for k, v in eval_results.items()
            if k != "execution_stats"
        }
        json.dump(serializable, f, indent=2, ensure_ascii=False)
    
    print(f"\nçµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_dir}")


if __name__ == "__main__":
    main()
