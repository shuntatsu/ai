"""
Binance OHLCVãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

Binance APIã‹ã‚‰è¤‡æ•°æ™‚é–“è»¸ï¼ˆ1m, 15m, 1h, 4h, 1dï¼‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€
æ—¥åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«åˆ†å‰²ä¿å­˜ã€‚
å‹•çš„ãªä¸Šä½é€šè²¨å–å¾—ï¼ˆ24h Quote VolumeåŸºæº–ï¼‰ã¨ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—æ©Ÿèƒ½ã«å¯¾å¿œã€‚
"""

import argparse
from datetime import datetime, timedelta
from pathlib import Path
import time
import json
import shutil
from typing import Dict, List, Optional
import pandas as pd
import requests


# å¯¾å¿œæ™‚é–“è»¸
TIMEFRAMES = ["1m", "15m", "1h", "4h", "1d"]

# Binanceã®å„ã‚·ãƒ³ãƒœãƒ«é–‹å§‹æ—¥ï¼ˆç´„ï¼‰- ä¸»è¦é€šè²¨ã®ã¿ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã€ä»–ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
SYMBOL_START_DATES = {
    "BTCUSDT": "2017-08-17",
    "ETHUSDT": "2017-08-17",
    "BNBUSDT": "2017-11-06",
    "SOLUSDT": "2020-08-11",
    "XRPUSDT": "2018-05-04",
    "DOGEUSDT": "2019-07-05",
    "ADAUSDT": "2018-04-17",
    "TRXUSDT": "2018-06-13",
    "AVAXUSDT": "2020-09-22",
    "LINKUSDT": "2019-01-16",
}

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé–‹å§‹æ—¥ï¼ˆä¸æ˜ãªã‚·ãƒ³ãƒœãƒ«ç”¨ï¼‰
DEFAULT_START_DATE = "2019-01-01"

# ä¸Šå ´æ—¥ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
LISTING_DATES_CACHE_FILE = Path("data/listing_dates.json")


def detect_listing_date(symbol: str, verbose: bool = True) -> Optional[str]:
    """
    äºŒåˆ†æ¢ç´¢ã§ã‚·ãƒ³ãƒœãƒ«ã®æœ€åˆã®å–å¼•æ—¥ã‚’æ¤œå‡º
    
    Args:
        symbol: å¯¾è±¡ã‚·ãƒ³ãƒœãƒ«
        verbose: ãƒ­ã‚°å‡ºåŠ›
        
    Returns:
        YYYY-MM-DDå½¢å¼ã®é–‹å§‹æ—¥ã€æ¤œå‡ºå¤±æ•—æ™‚ã¯None
    """
    # Binanceé–‹è¨­ï¼ˆ2017-07-14ï¼‰ã‹ã‚‰ä»Šæ—¥ã¾ã§
    min_ts = int(datetime(2017, 7, 14).timestamp() * 1000)
    max_ts = int(datetime.now().timestamp() * 1000)
    
    # ã¾ãšç¾åœ¨æ™‚åˆ»ã§ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã‹ç¢ºèªï¼ˆå»ƒæ­¢ã•ã‚ŒãŸé€šè²¨ãªã©ã®ãƒã‚§ãƒƒã‚¯ï¼‰
    latest = fetch_klines(symbol, "1M", limit=1) # æœˆè¶³ã§ãƒã‚§ãƒƒã‚¯
    if not latest:
        if verbose:
            print(f"âš ï¸ {symbol}: ç¾åœ¨ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“ï¼ˆå»ƒæ­¢ã¾ãŸã¯ç„¡åŠ¹ãªã‚·ãƒ³ãƒœãƒ«ï¼‰")
        return None

    # äºŒåˆ†æ¢ç´¢
    start_ts = min_ts
    end_ts = max_ts
    first_found_ts = None
    
    if verbose:
        print(f"ğŸ” {symbol}: ä¸Šå ´æ—¥ã‚’æ¤œç´¢ä¸­...", end="", flush=True)

    # 1ãƒ¶æœˆå˜ä½ãã‚‰ã„ã§å¤§ã¾ã‹ã«æ¢ã™
    while start_ts <= end_ts:
        mid_ts = (start_ts + end_ts) // 2
        
        # mid_tsä»¥é™ã®æœ€åˆã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        klines = fetch_klines(symbol, "1m", start_time=mid_ts, limit=1)
        
        if klines:
            # ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã£ãŸ -> ã‚‚ã£ã¨éå»ã«ã‚ã‚‹ã‹ã‚‚ã—ã‚Œãªã„
            kline_open_time = klines[0][0]
            first_found_ts = kline_open_time
            end_ts = mid_ts - 1
            # print(f".", end="", flush=True)
        else:
            # ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚‰ãªã„ -> ã‚‚ã£ã¨æœªæ¥ã«ã‚ã‚‹
            start_ts = mid_ts + 1
            # print(f".", end="", flush=True)
            
    if first_found_ts:
        dt = datetime.fromtimestamp(first_found_ts / 1000)
        date_str = dt.strftime("%Y-%m-%d")
        if verbose:
            print(f" ç™ºè¦‹! -> {date_str}")
        return date_str
    
    if verbose:
        print(" è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    return None


def load_listing_dates_cache() -> Dict[str, str]:
    if LISTING_DATES_CACHE_FILE.exists():
        try:
            with open(LISTING_DATES_CACHE_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return {}
    return {}


def save_listing_dates_cache(cache: Dict[str, str]):
    LISTING_DATES_CACHE_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(LISTING_DATES_CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(cache, f, indent=2)


def get_symbol_start_date(symbol: str, verbose: bool = True) -> str:
    """
    ã‚·ãƒ³ãƒœãƒ«ã®é–‹å§‹æ—¥ã‚’æ±ºå®šã™ã‚‹ã€‚
    1. ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸè¨­å®š
    2. ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    3. APIã§æ¤œå‡º
    4. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    ã®é †ã§æ±ºå®šã€‚
    """
    # 1. ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰
    if symbol in SYMBOL_START_DATES:
        return SYMBOL_START_DATES[symbol]
        
    # 2. ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    cache = load_listing_dates_cache()
    if symbol in cache:
        return cache[symbol]
        
    # 3. è‡ªå‹•æ¤œå‡º
    detected_date = detect_listing_date(symbol, verbose)
    if detected_date:
        cache[symbol] = detected_date
        save_listing_dates_cache(cache)
        return detected_date
        
    # 4. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    return DEFAULT_START_DATE


# é™¤å¤–ã™ã‚‹ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«ã‚³ã‚¤ãƒ³ç­‰ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰
EXCLUDE_SYMBOLS = {
    "USDT", "USDC", "BUSD", "DAI", "TUSD", "USDP", "USDD", "GUSD", 
    "LUSD", "SUSD", "FRAX", "MIM", "EURI", "PAXG", "WBTC", "FDUSD"
}

def get_binance_exchange_info() -> set:
    """Binanceã§å–å¼•å¯èƒ½ãªã‚·ãƒ³ãƒœãƒ«ã®ã‚»ãƒƒãƒˆã‚’å–å¾—"""
    url = "https://api.binance.com/api/v3/exchangeInfo"
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        data = response.json()
        return {s["symbol"] for s in data["symbols"] if s["status"] == "TRADING"}
    except Exception as e:
        print(f"âš ï¸ Error fetching exchange info: {e}")
        return set()


CoinGecko
    """
    CoinGeckoã‹ã‚‰æ™‚ä¾¡ç·é¡ä¸Šä½ã‚’å–å¾—ã—ã€Binanceã®USDTãƒšã‚¢ã«å¤‰æ›
    """
    print(f"\nğŸŒ Fetching top {limit} coins by Market Cap from CoinGecko...")
    
    # Binanceã®æœ‰åŠ¹ã‚·ãƒ³ãƒœãƒ«ã‚’å–å¾—ã—ã¦å­˜åœ¨ç¢ºèªã«ä½¿ç”¨
    binance_symbols = get_binance_exchange_info()
    if not binance_symbols:
        print("âš ï¸ Failed to verify Binance symbols. Falling back to simple conversion.")
    
    url = "https://api.coingecko.com/api/v3/coins/markets"
    params = {
        "vs_currency": "usd",
        "order": "market_cap_desc",
        "per_page": 200,  # é™¤å¤–åˆ†ã‚’è¦‹è¶Šã—ã¦å¤šã‚ã«å–å¾—
        "page": 1,
        "sparkline": "false"
    }
    
    try:
        response = requests.get(url, params=params, timeout=30)
        response.raise_for_status()
        data = response.json()
        
        valid_symbols = []
        rank = 1
        
        for coin in data:
            base_symbol = coin["symbol"].upper()
            symbol_usdt = f"{base_symbol}USDT"
            name = coin["name"]
            cap = coin.get("market_cap", 0)
            
            # --- ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° ---
            
            # 1. é™¤å¤–ãƒªã‚¹ãƒˆï¼ˆå®Œå…¨ä¸€è‡´ï¼‰
            if base_symbol in EXCLUDE_SYMBOLS:
                continue
                
            # 2. "USD" ã‚’å«ã‚€ã‚‚ã®ã‚’é™¤å¤–ï¼ˆã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«ã‚³ã‚¤ãƒ³å¯¾ç­–ï¼‰
            if "USD" in base_symbol:
                continue
                
            # 3. ãƒªã‚¹ãƒ†ãƒ¼ã‚­ãƒ³ã‚°/ãƒ©ãƒƒãƒ—ãƒ‰ç³»ãªã©ã®ç°¡æ˜“é™¤å¤–
            if base_symbol.startswith("W") and base_symbol != "WLD": # WBTC, WETHãªã©ã€‚WLDã¯é™¤å¤–ã—ãªã„
                 # WBTCã¯ä¸Šã®ãƒªã‚¹ãƒˆã§ã‚‚å¼¾ã„ã¦ã„ã‚‹ãŒå¿µã®ãŸã‚ã€‚
                 # ã¾ãWã ã‘ã ã¨èª¤çˆ†ã™ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã®ã§ã€ä¸»è¦ãªã‚‚ã®ã ã‘ã¯EXCLUDE_SYMBOLSã§ã€‚
                 pass
            
            # 4. Binanceå­˜åœ¨ãƒã‚§ãƒƒã‚¯
            if binance_symbols and symbol_usdt not in binance_symbols:
                continue

            # æ¡ç”¨
            valid_symbols.append(symbol_usdt)
            print(f"  {rank}. {base_symbol:<5} ({name[:15]:<15}) - Cap: ${cap:,.0f} -> {symbol_usdt}")
            rank += 1
            
            if len(valid_symbols) >= limit:
                break
                
        return valid_symbols
        
    except Exception as e:
        print(f"âŒ Error fetching from CoinGecko: {e}")
        print("Falling back to Volume based selection.")
        return get_top_symbols(limit)


def get_top_symbols(limit: int = 30, sort_by: str = "volume") -> List[str]:
    """
    ä¸Šä½ã‚·ãƒ³ãƒœãƒ«ã‚’å–å¾—
    
    Args:
        limit: å–å¾—æ•°
        sort_by: 'volume' or 'marketcap'
    """
    if sort_by == "marketcap":
        return fetch_top_by_market_cap(limit)

    # ä»¥ä¸‹ã€æ—¢å­˜ã®Volumeãƒ™ãƒ¼ã‚¹å‡¦ç†
    url = "https://api.binance.com/api/v3/ticker/24hr"
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        tickers = response.json()
        
        # USDTãƒšã‚¢ã®ã¿æŠ½å‡º & ãƒ¬ãƒãƒ¬ãƒƒã‚¸ãƒˆãƒ¼ã‚¯ãƒ³(UP/DOWN)ç­‰ã‚’é™¤å¤–
        filtered = []
        for t in tickers:
            symbol = t["symbol"]
            if not symbol.endswith("USDT"):
                continue
            
            # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            if any(k in symbol for k in ["UPUSDT", "DOWNUSDT", "BEARUSDT", "BULLUSDT", "BUSD", "DAI", "TUSD", "USDC"]):
                continue
                
            filtered.append(t)
            
        # å‡ºæ¥é«˜ï¼ˆquoteVolume = USDT Volumeï¼‰é †ã«ã‚½ãƒ¼ãƒˆ
        filtered.sort(key=lambda x: float(x["quoteVolume"]), reverse=True)
        
        # ä¸Šä½Nä»¶ã®ã‚·ãƒ³ãƒœãƒ«åã‚’æŠ½å‡º
        top_symbols = [t["symbol"] for t in filtered[:limit]]
        
        print(f"\nğŸ“Š Top {limit} Symbols by 24h Volume (USDT):")
        for i, s in enumerate(top_symbols, 1):
            # ã¡ã‚‡ã£ã¨ã—ãŸæƒ…å ±ã‚’è¡¨ç¤º
            t = next(filter(lambda x: x["symbol"] == s, filtered))
            vol = float(t["quoteVolume"])
            print(f"  {i}. {s:<10} (Vol: ${vol:,.0f})")
            
        return top_symbols
        
    except Exception as e:
        print(f"Error fetching top symbols: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ä¸»è¦é€šè²¨ã‚’è¿”ã™
        return ["BTCUSDT", "ETHUSDT", "SOLUSDT", "BNBUSDT", "XRPUSDT"]


def fetch_klines(
    symbol: str = "BTCUSDT",
    interval: str = "1m",
    start_time: int = None,
    end_time: int = None,
    limit: int = 1000,
) -> list:
    """
    Binance APIã‹ã‚‰Klineï¼ˆãƒ­ãƒ¼ã‚½ã‚¯è¶³ï¼‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    """
    url = "https://api.binance.com/api/v3/klines"
    
    params = {
        "symbol": symbol,
        "interval": interval,
        "limit": limit,
    }
    
    if start_time:
        params["startTime"] = start_time
    if end_time:
        params["endTime"] = end_time
    
    response = requests.get(url, params=params, timeout=30)
    response.raise_for_status()
    
    return response.json()


def klines_to_dataframe(klines: list) -> pd.DataFrame:
    """Klineãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã«å¤‰æ›"""
    if not klines:
        return pd.DataFrame()
    
    df = pd.DataFrame(klines, columns=[
        "open_time", "open", "high", "low", "close", "volume",
        "close_time", "quote_volume", "trades", 
        "taker_buy_base", "taker_buy_quote", "ignore"
    ])
    
    df["timestamp"] = df["open_time"].astype(int)
    df["open"] = df["open"].astype(float)
    df["high"] = df["high"].astype(float)
    df["low"] = df["low"].astype(float)
    df["close"] = df["close"].astype(float)
    df["volume"] = df["volume"].astype(float)
    
    df = df[["timestamp", "open", "high", "low", "close", "volume"]]
    
    return df


def fetch_one_day_data(
    symbol: str,
    interval: str,
    date: datetime,
    verbose: bool = False,
) -> pd.DataFrame:
    """1æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    start_time = int(date.replace(hour=0, minute=0, second=0, microsecond=0).timestamp() * 1000)
    end_time = int(date.replace(hour=23, minute=59, second=59, microsecond=999999).timestamp() * 1000)
    
    all_data = []
    current_start = start_time
    
    while current_start < end_time:
        try:
            klines = fetch_klines(
                symbol=symbol,
                interval=interval,
                start_time=current_start,
                end_time=end_time,
                limit=1000,
            )
            
            if not klines:
                break
            
            all_data.extend(klines)
            last_close_time = klines[-1][6]
            current_start = last_close_time + 1
            
            time.sleep(0.05)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
            
        except requests.exceptions.RequestException as e:
            if verbose:
                print(f"    ã‚¨ãƒ©ãƒ¼: {e}")
            time.sleep(1)
            continue
    
    return klines_to_dataframe(all_data)


def fetch_and_save_daily(
    symbol: str = "BTCUSDT",
    interval: str = "1m",
    start_date: str = None,
    days: int = None,
    output_dir: Path = None,
    verbose: bool = True,
) -> Dict[str, str]:
    """æ—¥åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«åˆ†å‰²ã—ã¦ä¿å­˜"""
    symbol_dir = output_dir / symbol / interval
    symbol_dir.mkdir(parents=True, exist_ok=True)
    
    # æœŸé–“ã®æ±ºå®š
    if start_date:
        start_dt = datetime.strptime(start_date, "%Y-%m-%d")
    else:
        start_dt = datetime.now() - timedelta(days=days or 30)
    
    if days:
        end_dt = start_dt + timedelta(days=days)
    else:
        end_dt = datetime.now()
    
    # å–å¾—æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—
    existing_dates = set()
    for f in symbol_dir.glob("*.csv"):
        try:
            existing_dates.add(f.stem)  # YYYY-MM-DD
        except:
            pass
    
    saved_files = {}
    current_dt = start_dt
    total_days = (end_dt - start_dt).days
    day_count = 0
    
    while current_dt < end_dt:
        date_str = current_dt.strftime("%Y-%m-%d")
        filepath = symbol_dir / f"{date_str}.csv"
        
        day_count += 1
        
        # ä»Šæ—¥ï¼ˆã¾ã çµ‚ã‚ã£ã¦ã„ãªã„æ—¥ï¼‰ã¯ã‚¹ã‚­ãƒƒãƒ—ã›ãšæœ€æ–°ã‚’å–å¾—ã—ã¦ã‚‚è‰¯ã„ãŒã€
        # ã“ã“ã§ã¯ã€Œéå»ã®ç¢ºå®šãƒ‡ãƒ¼ã‚¿ã€ã¨ã„ã†æ„å‘³ã§ã€æ—¢å­˜ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹æ–¹é‡ã¯ç¶­æŒ
        if date_str in existing_dates:
            if verbose:
                print(f"  [{day_count}/{total_days}] {date_str} ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ—¢å­˜ï¼‰")
            saved_files[date_str] = str(filepath)
            current_dt += timedelta(days=1)
            continue
        
        if verbose:
            print(f"  [{day_count}/{total_days}] {date_str} å–å¾—ä¸­...", end="")
        
        df = fetch_one_day_data(symbol, interval, current_dt, verbose=False)
        
        if len(df) > 0:
            df.to_csv(filepath, index=False)
            saved_files[date_str] = str(filepath)
            if verbose:
                print(f" {len(df):,}ãƒãƒ¼")
        else:
            if verbose:
                print(" ãƒ‡ãƒ¼ã‚¿ãªã—")
        
        current_dt += timedelta(days=1)
        time.sleep(0.1)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
    
    return saved_files


def fetch_multi_symbol_daily(
    symbols: List[str] = None,
    timeframes: List[str] = None,
    start_date: str = None,
    days: int = None,
    output_dir: Path = None,
    verbose: bool = True,
) -> Dict:
    """è¤‡æ•°ã‚·ãƒ³ãƒœãƒ«ãƒ»è¤‡æ•°æ™‚é–“è»¸ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ—¥åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã§ä¿å­˜"""
    if not symbols:
        return {}
        
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    results = {}
    total_symbols = len(symbols)
    
    for sym_idx, symbol in enumerate(symbols, 1):
        if verbose:
            print(f"\n{'='*60}")
            print(f"[{sym_idx}/{total_symbols}] {symbol}")
            print("="*60)
        
        results[symbol] = {}
        
        for tf in timeframes:
            if verbose:
                print(f"\n  {tf}:")
            
            # ã‚·ãƒ³ãƒœãƒ«å›ºæœ‰ã®é–‹å§‹æ—¥ã‚’ä½¿ç”¨
            effective_start = start_date
            if not effective_start and not days:
                effective_start = get_symbol_start_date(symbol, verbose=(tf == timeframes[0]))
            
            saved = fetch_and_save_daily(
                symbol=symbol,
                interval=tf,
                start_date=effective_start,
                days=days,
                output_dir=output_dir,
                verbose=verbose,
            )
            results[symbol][tf] = saved
            
    return results


def perform_cleanup(target_symbols: List[str], output_dir: Path):
    """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒªã‚¹ãƒˆã«ãªã„ã‚·ãƒ³ãƒœãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤"""
    print(f"\nğŸ§¹ Cleaning up data not in target list...")
    print(f"Target list ({len(target_symbols)}): {target_symbols[:5]}...")
    
    if not output_dir.exists():
        print("Output directory does not exist.")
        return

    # output_dirç›´ä¸‹ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒã‚§ãƒƒã‚¯
    removed_count = 0
    for item in output_dir.iterdir():
        if item.is_dir():
            symbol_name = item.name
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåãŒã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒªã‚¹ãƒˆã«ãªã‘ã‚Œã°å‰Šé™¤
            if symbol_name not in target_symbols:
                print(f"  - Removing: {symbol_name}")
                try:
                    shutil.rmtree(item)
                    removed_count += 1
                except Exception as e:
                    print(f"    Error removing {symbol_name}: {e}")
    
    print(f"Done. Removed {removed_count} directories.")


def main():
    parser = argparse.ArgumentParser(description="Binance OHLCVãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆæ—¥åˆ¥ä¿å­˜ãƒ»è¤‡æ•°é€šè²¨å¯¾å¿œï¼‰")
    parser.add_argument("--symbol", type=str, default=None, help="é€šè²¨ãƒšã‚¢ï¼ˆå˜ä¸€æŒ‡å®šï¼‰")
    parser.add_argument("--symbols", type=str, nargs="+", default=None, help="è¤‡æ•°é€šè²¨ãƒšã‚¢æŒ‡å®š")
    parser.add_argument("--top", type=int, default=None, help="ä¸Šä½Né€šè²¨ã‚’è‡ªå‹•å–å¾—")
    parser.add_argument("--sort", type=str, default="volume", choices=["volume", "marketcap"], help="ã‚½ãƒ¼ãƒˆåŸºæº– (volume/marketcap)")
    parser.add_argument("--interval", type=str, default=None, help="å˜ä¸€æ™‚é–“è¶³")
    parser.add_argument("--timeframes", type=str, nargs="+", default=None, help="è¤‡æ•°æ™‚é–“è»¸")
    parser.add_argument("--days", type=int, default=None, help="å–å¾—æ—¥æ•°")
    parser.add_argument("--start-date", type=str, default=None, help="é–‹å§‹æ—¥ YYYY-MM-DD")
    parser.add_argument("--all", action="store_true", dest="fetch_all", help="å…¨æœŸé–“å–å¾—")
    parser.add_argument("--output", type=str, default="./data", help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument("--multi", action="store_true", help="å¤šæ™‚é–“è»¸ãƒ¢ãƒ¼ãƒ‰")
    parser.add_argument("--clean", action="store_true", help="ãƒªã‚¹ãƒˆå¤–ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã™ã‚‹")
    args = parser.parse_args()
    
    output_dir = Path(args.output)
    
    # ã‚·ãƒ³ãƒœãƒ«ãƒªã‚¹ãƒˆã®æ±ºå®š
    if args.symbols:
        symbols = args.symbols
    elif args.top:
        symbols = get_top_symbols(limit=args.top, sort_by=args.sort)
    elif args.symbol:
        symbols = [args.symbol]
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãªã—ï¼ˆ--topãªã©ã‚’å¼·åˆ¶ã—ãŸã„ãŒã€äº’æ›æ€§ã®ãŸã‚BTCã®ã¿ï¼‰
        print("âš ï¸ No symbols specified. Using BTCUSDT.")
        symbols = ["BTCUSDT"]
    
    # ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–
    symbols = list(set(symbols))
    
    # æ™‚é–“è»¸ã®æ±ºå®š
    if args.timeframes:
        timeframes = args.timeframes
    elif args.interval:
        timeframes = [args.interval]
    elif args.multi:
        timeframes = TIMEFRAMES
    else:
        timeframes = ["1m"]
    
    # å…¨æœŸé–“ãƒ¢ãƒ¼ãƒ‰
    start_date = args.start_date
    days = args.days
    if args.fetch_all:
        start_date = None  # fetch_and_save_dailyã§ã‚·ãƒ³ãƒœãƒ«å›ºæœ‰ã®é–‹å§‹æ—¥ã‚’ä½¿ç”¨
        days = None
        print("\nâœ¨ å…¨æœŸé–“ãƒ¢ãƒ¼ãƒ‰: å„ã‚·ãƒ³ãƒœãƒ«ã®å–å¼•é–‹å§‹æ—¥ã‹ã‚‰å–å¾—ã—ã¾ã™")
    elif not start_date and not days:
        days = 30  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ30æ—¥
    
    print(f"\nå¯¾è±¡ã‚·ãƒ³ãƒœãƒ«: {len(symbols)}é€šè²¨")
    print(f"å¯¾è±¡æ™‚é–“è»¸: {timeframes}")
    print(f"å‡ºåŠ›å…ˆ: {output_dir}")
    
    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‰ã«å®Ÿè¡Œã—ã¦ã€ä¸è¦ãªã‚‚ã®ã‚’æ¶ˆã™ï¼‰
    # ã‚ã‚‹ã„ã¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¾Œã«å®Ÿè¡Œã™ã‚‹ã‹ï¼Ÿ
    # -> å¼•æ•°ã§æŒ‡å®šã•ã‚ŒãŸãƒªã‚¹ãƒˆãŒã€Œæ­£ã€ãªã®ã§ã€ã“ã“ã«å«ã¾ã‚Œãªã„ã‚‚ã®ã¯æ¶ˆã™ã€‚
    if args.clean:
        perform_cleanup(symbols, output_dir)
    
    # å–å¾—å®Ÿè¡Œ
    fetch_multi_symbol_daily(
        symbols=symbols,
        timeframes=timeframes,
        start_date=start_date,
        days=days,
        output_dir=output_dir,
        verbose=True,
    )
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    metadata = {
        "symbols": symbols,
        "timeframes": timeframes,
        "fetch_time": datetime.now().isoformat(),
        "structure": "daily_files",
        "path_pattern": "{symbol}/{interval}/YYYY-MM-DD.csv",
    }
    
    with open(output_dir / "metadata.json", "w", encoding="utf-8") as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    print("\n" + "="*60)
    print("å®Œäº†!")
    print("="*60)


if __name__ == "__main__":
    main()
